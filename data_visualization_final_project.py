# -*- coding: utf-8 -*-
"""Data Visualization Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Uh4ZKepLRgFWjAGabiAAEmROrI_rkyb

***Data Visualization Final Project***




> ```
Jobayrul Hasan || 251-25-001
Naima Azim || 251-25-008
Shamsia Afrin Jamema || 251-25-010
Mohammed Daud || 251-25-027
Md. Naib Hossain Khan || 251-25-039
Mohammad Abdur Rahman || 251-25-040
```

**Important Libraries**
"""

import pandas as pd
import numpy as np
import gdown
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from matplotlib.ticker import FuncFormatter
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import plotly.express as px
import plotly.graph_objects as go

"""**Dataset Load**"""

file_id = "1c1zUQfxuHURG1Y7n_aH90V0kEG2ws7Pa"
file_url = f"https://drive.google.com/uc?id={file_id}"
output = "data.csv"

gdown.download(file_url, output, quiet=False)
df = pd.read_csv(output)

"""# **Data Pre-Processing: Afrin**

**Explore Dataset**
"""

df.head()

df.shape

df.columns

df.dtypes

df.isnull().sum()

"""**Data Preprocess**"""

df["IsTotal"] = df["Unit"].str.lower() == "Total"

df["Year"] = df["Year"].astype(int)
df["Month"] = df["Month"].str.strip().str.title()
df["Unit"] = df["Unit"].str.strip().str.title()
df["Total Cases"] = df["Total Cases"].fillna(0).astype(int)

month_order = [
    "January", "February", "March", "April", "May", "June",
    "July", "August", "September", "October", "November", "December"
]
df["Month"] = pd.Categorical(df["Month"], categories=month_order, ordered=True)

df["Date"] = pd.to_datetime(
    df["Year"].astype(str) + "-" + df["Month"].astype(str),
    format="%Y-%B", errors="coerce"
).dt.to_period("M").astype(str)

invalid_dates = df[df["Date"].isna()]
print("Invalid or Missing Dates:")
print(invalid_dates)

df = df.sort_values(by=["Year", "Month"]).reset_index(drop=True)

print(df.head(10))

df.columns

df.shape

total_rows = df[df['IsTotal']].copy()
crime_data = df[~df['IsTotal']].copy()

for idx, row in total_rows.iterrows():
    calculated_total = crime_data[(crime_data['Year'] == row['Year']) &
                       (crime_data['Month'] == row['Month'])][crime_columns].sum().sum()
    print(f"Year: {row['Year']}, Month: {row['Month']}")
    print(f"Recorded Total: {row['Total Cases']}, Calculated Total: {calculated_total}")
    print("-"*50)

crime_columns = [
    'Dacoity', 'Robbery', 'Murder', 'Speedy Trial', 'Riot',
    'Woman & Child Repression', 'Kidnapping', 'Police Assault',
    'Burglary', 'Theft', 'Arms Act', 'Explosive Act',
    'Narcotics', 'Smuggling'
]

for col in crime_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)

"""# **Quick Static Visualizations: Naima**

**Monthly Crime Trends (2019-2025)**
"""

try:
    crime_columns
except NameError:
    raise NameError("Run the cell where 'crime_columns' is defined first!")

df['Date'] = pd.to_datetime(df['Date'])
df = df.set_index('Date')
crime_monthly = df[crime_columns].resample('M').sum()

avg_monthly = crime_monthly.groupby(crime_monthly.index.month).mean()
avg_monthly.index = pd.date_range("2000-01-01", periods=12, freq="MS").strftime("%B")
monthly_totals = avg_monthly.sum(axis=1)

plt.figure(figsize=(12, 6))
sns.lineplot(data=monthly_totals, marker='o', linewidth=2,
            color='teal', label='Monthly Crime Cases')

mean_cases = monthly_totals.mean()
plt.axhline(mean_cases, color='red', linestyle='--',
           label=f'Yearly Average: {mean_cases:,.0f} Cases')

plt.title('Average Monthly Crime Trends (2019â€“2025)')
plt.ylabel('Average Cases')
plt.xlabel('Month')
plt.xticks(rotation=45)
plt.grid(alpha=0.3)
plt.legend()
plt.tight_layout()
plt.show()

"""**Year-Over-Year Comparison**"""

def prepare_yoy_data(df, crime_columns):
    if 'Date' not in df.columns:
        df['Date'] = pd.to_datetime(
            df['Year'].astype(str) + '-' +
            df['Month'].astype(str) + '-01',
            errors='coerce'
        )
        df = df.dropna(subset=['Date'])

    df = df.set_index('Date')
    return df[crime_columns].resample('Y').sum()

def plot_yoy_comparison(yearly_data):
    plt.figure(figsize=(12, 6))
    for year in yearly_data.index.year:
        year_data = yearly_data[yearly_data.index.year == year]
        sns.lineplot(
            x=year_data.columns,
            y=year_data.values.flatten(),
            marker='o',
            label=str(year),
            linewidth=2
        )

    plt.title('Year-over-Year Crime Comparison (2019-2025)', fontsize=14, pad=15)
    plt.xlabel('Crime Categories', fontsize=12)
    plt.ylabel('Total Cases', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.grid(alpha=0.3)
    plt.legend(title='Year')

    def format_func(value, _):
        if value >= 1e6:
            return f'{value/1e6:.1f}M'
        if value >= 1e3:
            return f'{value/1e3:.0f}K'
        return f'{value:.0f}'

    plt.gca().yaxis.set_major_formatter(FuncFormatter(format_func))
    plt.tight_layout()
    plt.show()

try:
    crime_columns
    yearly_data = prepare_yoy_data(df.copy(), crime_columns)
    plot_yoy_comparison(yearly_data)
except NameError:
    print("Error: Please define crime_columns first")
except Exception as e:
    print(f"An error occurred: {str(e)}")

"""**Crime by Police Unit (Top 5)**"""

filtered_df = df[df['IsTotal'] == False].copy()
filtered_df = filtered_df[~filtered_df['Unit'].str.contains('total', case=False)]
filtered_df = filtered_df.dropna(subset=['Unit'])
unit_crimes = filtered_df.groupby('Unit')[crime_columns].sum().sum(axis=1)
top5_units = unit_crimes.nlargest(5)

for unit in top5_units.index:
    is_total = any(df[df['Unit'] == unit]['IsTotal'])

plt.figure(figsize=(12, 6))
ax = sns.barplot(
    x=top5_units.values,
    y=top5_units.index,
    palette="flare",
    saturation=0.9
)

for i, (unit, cases) in enumerate(zip(top5_units.index, top5_units.values)):
    if df[df['Unit'] == unit]['IsTotal'].any():
        raise ValueError(f"CRITICAL ERROR: Total unit '{unit}' is being plotted!")

    ax.text(
        cases + max(top5_units.values)*0.02,
        i,
        f"{cases:,.0f}",
        va='center',
        fontweight='bold'
    )

plt.title("Top 5 Police Units\n2019-2025",
         fontsize=14, fontweight='bold')
plt.xlabel("Total Cases", fontsize=12)
plt.ylabel("")
sns.despine()
plt.tight_layout()
plt.show()

"""**Regional Heatmap**"""

region_pivot = df.pivot_table(index='Unit', columns='Year', values='Total Cases', aggfunc='sum')
plt.figure(figsize=(12, 8))
sns.heatmap(region_pivot, annot=True, fmt=".0f", cmap="YlGnBu")
plt.title('Regional Crime Heatmap', fontsize=14)
plt.show()

"""**Top Crime Categories**"""

sns.set_style("whitegrid")
plt.figure(figsize=(10, 5))
crime_data = df[crime_columns].sum()
top5_crimes = crime_data.nlargest(5)
colors = sns.color_palette("YlOrRd_r", n_colors=5)
ax = sns.barplot(x=top5_crimes.values,
                y=top5_crimes.index,
                palette=colors,
                edgecolor="black",
                linewidth=0.5)

for i, value in enumerate(top5_crimes):
    ax.text(value + max(top5_crimes.values)*0.02,
           i,
           f"{value:,.0f}",
           va="center",
           fontweight="bold",
           color="darkred")

plt.title("Top 5 Crime Categories (2019-2025)",
         fontsize=14, pad=15, fontweight="bold")
plt.xlabel("Reported Cases", fontsize=12)
plt.ylabel("")
plt.xticks(fontsize=10)
plt.yticks(fontsize=11)
sns.despine(left=True, bottom=True)
plt.tight_layout()
plt.show()

"""**Woman & Child Repression Cases**"""

monthly_full = df[crime_columns].resample('M').sum()

repression_cols = [
    col for col in monthly_full.columns
    if any(keyword in col.lower()
          for keyword in ['woman', 'child', 'repression', 'violence', 'abuse'])
]

plt.figure(figsize=(14, 7))
palette = sns.color_palette("Reds_r", len(repression_cols))

for idx, col in enumerate(repression_cols):
    sns.lineplot(
        data=monthly_full[col],
        label=f"{col} (Peak: {monthly_full[col].max():,.0f})",
        linewidth=2.5,
        color=palette[idx],
        marker='o',
        markersize=8,
        markevery=3
    )

plt.title('Woman & Child Repression Cases\nMonthly Trends (2019â€“2025)',
         fontsize=16, pad=20, fontweight='bold')
plt.ylabel('Reported Cases', fontsize=12)
plt.xlabel('Timeline', fontsize=12)
plt.grid(axis='y', alpha=0.3)

legend = plt.legend(
    bbox_to_anchor=(1.05, 1),
    loc='upper right',
    frameon=True,
    framealpha=0.9
)
legend.set_title('Case Types (with Peak Values)', prop={'weight':'bold'})

max_month = monthly_full[repression_cols].sum(axis=1).idxmax()
plt.axvline(
    x=max_month,
    color='darkred',
    linestyle='--',
    alpha=0.5,
    label=f'Worst Month: {max_month.strftime("%b %Y")}'
)

plt.tight_layout()
plt.show()

"""**Crime Category Clustering**"""

scaler = StandardScaler()
scaled_data = scaler.fit_transform(monthly_full.T)
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(12, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Cluster Number', fontsize=14)
plt.xlabel('Number of Clusters')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.grid(True)
plt.show()

optimal_clusters = 3
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

cluster_df = pd.DataFrame({
    'Crime': monthly_full.columns,
    'Cluster': clusters,
    'Centroid_Distance': np.min(kmeans.transform(scaled_data), axis=1)
}).sort_values(['Cluster', 'Centroid_Distance'])

plt.figure(figsize=(14, 8))
scatter = sns.scatterplot(
    data=cluster_df,
    x='Centroid_Distance',
    y='Crime',
    hue='Cluster',
    palette='viridis',
    s=150,
    style='Cluster',
    markers=['o', 's', 'D', '^', 'P'][:optimal_clusters]
)

cluster_interpretations = {
    0: "Seasonal/Periodic Crimes",
    1: "Consistently High Crimes",
    2: "Low-Frequency Crimes"
}

for cluster_num in range(optimal_clusters):
    plt.axvline(
        x=cluster_df[cluster_df['Cluster'] == cluster_num]['Centroid_Distance'].mean(),
        color='gray',
        linestyle='--',
        alpha=0.5
    )
    plt.text(
        x=cluster_df[cluster_df['Cluster'] == cluster_num]['Centroid_Distance'].max(),
        y=len(monthly_full.columns) * 0.8 - cluster_num * 3,
        s=cluster_interpretations.get(cluster_num, f"Cluster {cluster_num}"),
        fontsize=12,
        fontweight='bold',
        color=plt.get_cmap('viridis')(cluster_num/optimal_clusters)
    )

plt.title(f'Crime Category Clustering (k={optimal_clusters})\nPattern Similarity Analysis',
         fontsize=16, pad=20)
plt.xlabel('Distance from Cluster Centroid (Standardized)', fontsize=12)
plt.ylabel('Crime Category', fontsize=12)
plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')
plt.grid(axis='x', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

"""# **Interactive Time-Series and Regional Comparisons: Riad**

**Yearly Crime Trends by Category**
"""

df = df[~df["IsTotal"]]
crime_categories = [
    'Dacoity', 'Robbery', 'Murder', 'Speedy Trial', 'Riot',
    'Woman & Child Repression', 'Kidnapping', 'Police Assault',
    'Burglary', 'Theft', 'Arms Act',
    'Explosive Act', 'Narcotics', 'Smuggling'
]
df_melt = df.melt(
    id_vars=["Year", "Unit"],
    value_vars=crime_categories,
    var_name="Crime Type",
    value_name="Cases"
)
fig = px.line(
    df_melt,
    x="Year",
    y="Cases",
    color="Crime Type",
    title="Yearly Crime Trends by Category",
    markers=True
)
fig.show()

"""**Top 5 Crime Categories**"""

year_selected = 2024
df_year = df[df["Year"] == year_selected]

top5 = df_year[crime_categories].sum().nlargest(5).reset_index()
top5.columns = ["Crime Type", "Cases"]

fig = px.pie(
    top5,
    names="Crime Type",
    values="Cases",
    title=f"Top 5 Crime Categories - {year_selected}",
    hole=0.3
)
fig.show()

"""**Year-over-Year Change in Total Cases**"""

df_yearly = df.groupby("Year")["Total Cases"].sum().pct_change() * 100
df_yearly = df_yearly.reset_index(name="YoY % Change")

fig = px.bar(
    df_yearly,
    x="Year",
    y="YoY % Change",
    title="Year-over-Year Change in Total Cases",
    text_auto=".2f"
)
fig.show()

"""**Crime Seasonality (Total Cases per Month)**"""

df_monthly = df.groupby("Month")["Total Cases"].sum().reset_index()
fig = px.bar(
    df_monthly,
    x="Month",
    y="Total Cases",
    title="Crime Seasonality (Total Cases per Month)",
    text_auto=True
)
fig.show()

"""**Crime Profile (Average Cases)**"""

crime_categories = [
    'Dacoity', 'Robbery', 'Murder', 'Speedy Trial', 'Riot',
    'Woman & Child Repression', 'Kidnapping', 'Police Assault',
    'Burglary', 'Theft', 'Arms Act',
    'Explosive Act', 'Narcotics', 'Smuggling'
]
df_parallel = df.groupby("Unit")[crime_categories].mean().reset_index()

fig = px.parallel_coordinates(
    df_parallel,
    color="Murder",
    dimensions=crime_categories,
    title="Crime Profile (Average Cases)"
)
fig.show()

"""**Narcotics vs Arms Act Cases**"""

fig = px.scatter(
    df,
    x="Narcotics",
    y="Arms Act",
    color="Year",
    size="Total Cases",
    hover_name="Unit",
    title="Narcotics vs Arms Act Cases"
)
fig.show()

"""**Yearly Crime Trends (Faceted by Type)**"""

df = df[~df["IsTotal"]]
crime_categories = [
    'Dacoity', 'Robbery', 'Murder', 'Speedy Trial', 'Riot',
    'Woman & Child Repression', 'Kidnapping', 'Police Assault',
    'Burglary', 'Theft', 'Arms Act',
    'Explosive Act', 'Narcotics', 'Smuggling'
]
df_melt = df.melt(
    id_vars=["Year", "Unit"],
    value_vars=crime_categories,
    var_name="Crime Type",
    value_name="Cases"
)
fig = px.line(
    df_melt,
    x="Year",
    y="Cases",
    color="Unit",
    facet_col="Crime Type",
    facet_col_wrap=4,
    title="Yearly Crime Trends (Faceted by Type)"
)
fig.show()

"""**Crime Type Race Over Time**"""

crime_categories = [c for c in df.columns if c not in [
    "Year", "Month", "Unit", "IsTotal", "Total Cases", "Total Recovery Cases", "Other Cases"
]]

df_filtered = df[~df["IsTotal"]]
df_melt = df_filtered.melt(id_vars=["Year"], value_vars=crime_categories,
                           var_name="Crime Type", value_name="Cases")

df_grouped = df_melt.groupby(["Year", "Crime Type"])["Cases"].sum().reset_index()

fig = px.bar(
    df_grouped,
    x="Cases",
    y="Crime Type",
    color="Crime Type",
    orientation="h",
    animation_frame="Year",
    title="Crime Type Race Over Time"
)
fig.show()

"""**Crime Hotspot Bubble Map**"""

unit_city_map = {
    "Dmp": "Dhaka", "Cmp": "Chattogram", "Kmp": "Khulna",
    "Rmp": "Rajshahi", "Bmp": "Barishal", "Smp": "Sylhet",
    "Rpmp": "Rangpur", "Gmp": "Gazipur",
    "Dhaka Range": "Dhaka Division", "Mymensingh Range": "Mymensingh Division",
    "Chittagong Range": "Chattogram Division", "Sylhet Range": "Sylhet Division",
    "Khulna Range": "Khulna Division", "Barishal Range": "Barishal Division",
    "Rajshahi Range": "Rajshahi Division", "Rangpur Range": "Rangpur Division",
    "Railway Range": "Railway"
}

df_filtered = df[~df["IsTotal"]].copy()
df_filtered["City"] = df_filtered["Unit"].map(unit_city_map)

df_city = df_filtered.groupby(["Year", "City"])["Total Cases"].sum().reset_index()

city_coords = {
    "Dhaka": [23.8103, 90.4125], "Chattogram": [22.3569, 91.7832], "Khulna": [22.8456, 89.5403],
    "Rajshahi": [24.3745, 88.6042], "Barishal": [22.7010, 90.3535], "Sylhet": [24.8949, 91.8687],
    "Rangpur": [25.7439, 89.2752], "Gazipur": [23.9999, 90.4203]
}

df_city["lat"] = df_city["City"].map(lambda x: city_coords.get(x, [None, None])[0])
df_city["lon"] = df_city["City"].map(lambda x: city_coords.get(x, [None, None])[1])

fig = px.scatter_mapbox(
    df_city,
    lat="lat",
    lon="lon",
    size="Total Cases",
    color="City",
    animation_frame="Year",
    size_max=50,
    zoom=5,
    mapbox_style="carto-positron",
    title="Crime Hotspot Bubble Map"
)
fig.show()

"""**Crime Seasonality Heatmap**"""

df_filtered = df[~df["IsTotal"]]
df_monthly = df_filtered.groupby(["Year", "Month"])["Total Cases"].sum().reset_index()
fig = px.density_heatmap(
    df_monthly,
    x="Month",
    y="Year",
    z="Total Cases",
    color_continuous_scale="Inferno",
    title="Crime Seasonality Heatmap"
)
fig.show()

"""**Crimes Over The Time**"""

crime_types = df.columns[3:-1]
fig = go.Figure()

for i, crime in enumerate(crime_types):
    trace = go.Scatter(
        x=df.groupby('Date')[crime].sum().index,
        y=df.groupby('Date')[crime].sum().values,
        mode='lines+markers',
        name=crime,
        visible=(i == 0)
    )
    fig.add_trace(trace)

buttons = []
for i, crime in enumerate(crime_types):
    visibility = [False] * len(crime_types)
    visibility[i] = True
    buttons.append(dict(label=crime,
                        method='update',
                        args=[{'visible': visibility},
                              {'title': f"{crime} Over Time"}]))
fig.update_layout(
    title=f"{crime_types[0]} Over Time",
    updatemenus=[dict(active=0, buttons=buttons, x=1.15, y=1.15)],
    xaxis_title="Date",
    yaxis_title="Number of Cases"
)

fig.show()

"""Machine Learning"""

Ml_df = df.copy()
Ml_df["Date"] = pd.to_datetime(
    Ml_df["Year"].astype(str) + "-" + Ml_df["Month"].astype(str),
    format="%Y-%B", errors="coerce"
).dt.to_period("M").astype(str)
Ml_df = Ml_df.dropna(subset=['Date'])
Ml_df = Ml_df.sort_values(by=["Year", "Month"]).reset_index(drop=True)
Ml_df = Ml_df.set_index('Date')
Ml_df = Ml_df[~Ml_df['IsTotal']].copy()
print(Ml_df.head())

"""Prepare data for prediction"""

crime_columns = [
    'Dacoity', 'Robbery', 'Murder', 'Speedy Trial', 'Riot',
    'Woman & Child Repression', 'Kidnapping', 'Police Assault',
    'Burglary', 'Theft', 'Arms Act', 'Explosive Act',
    'Narcotics', 'Smuggling'
]

Ml_df_reset = Ml_df.reset_index()

Ml_df_time_series = Ml_df_reset[['Unit', 'Date'] + crime_columns].copy()

Ml_df_time_series['Date'] = pd.to_datetime(Ml_df_time_series['Date'], errors='coerce')

Ml_df_time_series = Ml_df_time_series.dropna(subset=['Date'])

Ml_df_time_series = Ml_df_time_series.sort_values(by=['Unit', 'Date']).reset_index(drop=True)


# Group by the original 'Unit' column from Ml_df_reset and then resample
Ml_df_time_series_filtered = Ml_df_time_series.groupby('Unit').resample('ME', on='Date')[crime_columns].sum().fillna(0)

Ml_df_time_series_filtered = Ml_df_time_series_filtered.reset_index()

print(Ml_df_time_series_filtered.head())

"""Choose and train predictive model"""

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import warnings
import numpy as np

trained_models = {}
successful_models = []
skipped_models = []

# Focus on aggregate data (Total) and major crime categories for better results
priority_units = ['Total']
priority_crimes = ['Total Cases', 'Murder', 'Robbery', 'Theft', 'Narcotics']

print("Starting model training with improved approach...")

for unit in Ml_df_time_series_filtered['Unit'].unique():
    # Prioritize Total data and major units
    if unit not in priority_units and len(Ml_df_time_series_filtered[Ml_df_time_series_filtered['Unit'] == unit]) < 12:
        continue  # Skip units with insufficient data
    
    for crime in crime_columns:
        # Prioritize major crime categories
        if crime not in priority_crimes and unit not in priority_units:
            continue
            
        unit_crime_data = Ml_df_time_series_filtered[
            Ml_df_time_series_filtered['Unit'] == unit
        ].set_index('Date')[crime]

        # Ensure the index is a DatetimeIndex and has a frequency
        unit_crime_data.index = pd.to_datetime(unit_crime_data.index)
        unit_crime_data = unit_crime_data.asfreq('ME').fillna(0)
        
        # Check if we have enough data (at least 12 observations)
        if len(unit_crime_data) < 12:
            skipped_models.append(f"{unit}-{crime}-Insufficient_Data")
            continue
            
        # Remove any remaining NaN values
        unit_crime_data = unit_crime_data.dropna()
        
        if len(unit_crime_data) < 6:
            skipped_models.append(f"{unit}-{crime}-Too_Few_Observations")
            continue

        # Try multiple modeling approaches
        model_success = False
        
        # Approach 1: Simple ARIMA with basic orders
        arima_orders_to_try = [(1, 0, 0), (0, 0, 1), (1, 0, 1), (0, 1, 0)]
        
        for order in arima_orders_to_try:
            try:
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore")
                    
                    model = ARIMA(unit_crime_data, order=order)
                    model_fit = model.fit()
                    
                    # Basic validation - check if model has reasonable parameters
                    if hasattr(model_fit, 'aic') and not np.isnan(model_fit.aic):
                        trained_models[(unit, crime, 'ARIMA')] = model_fit
                        successful_models.append(f"{unit}-{crime}-ARIMA{order}")
                        model_success = True
                        break
                        
            except Exception as e:
                continue
        
        # Approach 2: Exponential Smoothing if ARIMA fails
        if not model_success:
            try:
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore")
                    
                    # Use simple exponential smoothing
                    model = ExponentialSmoothing(unit_crime_data, trend='add', seasonal=None)
                    model_fit = model.fit()
                    
                    trained_models[(unit, crime, 'ETS')] = model_fit
                    successful_models.append(f"{unit}-{crime}-ETS")
                    model_success = True
                    
            except Exception as e:
                pass
        
        # Approach 3: Simple Linear Regression as fallback
        if not model_success:
            try:
                # Create time-based features
                X = np.arange(len(unit_crime_data)).reshape(-1, 1)
                y = unit_crime_data.values
                
                model = LinearRegression()
                model_fit = model.fit(X, y)
                
                # Store the model with additional info for prediction
                model_info = {
                    'model': model_fit,
                    'data': unit_crime_data,
                    'type': 'Linear'
                }
                trained_models[(unit, crime, 'Linear')] = model_info
                successful_models.append(f"{unit}-{crime}-Linear")
                model_success = True
                
            except Exception as e:
                pass
        
        # Approach 4: Random Forest as last resort
        if not model_success:
            try:
                # Create time-based features
                X = np.arange(len(unit_crime_data)).reshape(-1, 1)
                y = unit_crime_data.values
                
                model = RandomForestRegressor(n_estimators=10, random_state=42)
                model_fit = model.fit(X, y)
                
                # Store the model with additional info for prediction
                model_info = {
                    'model': model_fit,
                    'data': unit_crime_data,
                    'type': 'RandomForest'
                }
                trained_models[(unit, crime, 'RandomForest')] = model_info
                successful_models.append(f"{unit}-{crime}-RandomForest")
                model_success = True
                
            except Exception as e:
                pass
        
        if not model_success:
            skipped_models.append(f"{unit}-{crime}-All_Models_Failed")

# Summary statistics
print(f"\n=== Model Training Summary ===")
print(f"Total models attempted: {len(successful_models) + len(skipped_models)}")
print(f"Successfully trained: {len(successful_models)}")
print(f"Failed/Skipped: {len(skipped_models)}")
print(f"Success rate: {len(successful_models)/(len(successful_models) + len(skipped_models))*100:.1f}%")

print(f"\nSuccessful Model Training: {', '.join(successful_models[:10])}")  # Show first 10
if len(successful_models) > 10:
    print(f"... and {len(successful_models) - 10} more successful models")

print(f"\nSkipped Model Training: {', '.join(skipped_models[:10])}")  # Show first 10
if len(skipped_models) > 10:
    print(f"... and {len(skipped_models) - 10} more skipped models")

# Show some example predictions
print(f"\n=== Sample Predictions ===")
prediction_count = 0
for (unit, crime, model_type), model in trained_models.items():
    if prediction_count >= 5:  # Show only first 5 predictions
        break
        
    try:
        if model_type == 'ARIMA':
            forecast = model.forecast(steps=3)
            print(f"{unit}-{crime} (ARIMA): Next 3 months forecast = {forecast.values}")
        elif model_type == 'ETS':
            forecast = model.forecast(steps=3)
            print(f"{unit}-{crime} (ETS): Next 3 months forecast = {forecast}")
        elif model_type in ['Linear', 'RandomForest']:
            # For ML models, we need to create future time points
            last_idx = len(model['data'])
            future_X = np.arange(last_idx, last_idx + 3).reshape(-1, 1)
            forecast = model['model'].predict(future_X)
            print(f"{unit}-{crime} ({model_type}): Next 3 months forecast = {forecast}")
        
        prediction_count += 1
        
    except Exception as e:
        print(f"Error making prediction for {unit}-{crime}: {str(e)}")
        continue

"""# **Predict Top 5 Crimes for Next Month and Generate Pie Chart**"""

from datetime import datetime, timedelta
import calendar

# Get current date and calculate next month
current_date = datetime.now()
current_month = current_date.month
current_year = current_date.year

# Calculate next month
if current_month == 12:
    next_month = 1
    next_year = current_year + 1
else:
    next_month = current_month + 1
    next_year = current_year

print(f"Current Date: {current_date.strftime('%B %Y')}")
print(f"Predicting for: {calendar.month_name[next_month]} {next_year}")

# Create a dictionary to store predictions for each unit
unit_predictions = {}

# Generate predictions for each unit and crime type
for (unit, crime, model_type), model in trained_models.items():
    try:
        if model_type == 'ARIMA':
            # Get the next month prediction (index 0 for next month)
            forecast = model.forecast(steps=1)
            prediction = forecast.values[0]
        elif model_type == 'ETS':
            forecast = model.forecast(steps=1)
            prediction = forecast[0]
        elif model_type in ['Linear', 'RandomForest']:
            # For ML models, predict next month
            last_idx = len(model['data'])
            future_X = np.array([[last_idx]])
            prediction = model['model'].predict(future_X)[0]
        
        # Store prediction (ensure it's non-negative)
        prediction = max(0, prediction)
        
        if unit not in unit_predictions:
            unit_predictions[unit] = {}
        unit_predictions[unit][crime] = prediction
        
    except Exception as e:
        print(f"Error predicting for {unit}-{crime}: {str(e)}")
        continue

# Function to get top 5 crimes for each unit
def get_top_5_crimes(unit_predictions):
    top_crimes_by_unit = {}
    
    for unit, crimes in unit_predictions.items():
        # Sort crimes by predicted values (descending)
        sorted_crimes = sorted(crimes.items(), key=lambda x: x[1], reverse=True)
        # Get top 5
        top_5 = sorted_crimes[:5]
        top_crimes_by_unit[unit] = top_5
    
    return top_crimes_by_unit

# Get top 5 crimes for each unit
top_crimes_by_unit = get_top_5_crimes(unit_predictions)

# Print predictions for each unit
print(f"\n=== Top 5 Crime Predictions for {calendar.month_name[next_month]} {next_year} ===")
for unit, top_crimes in top_crimes_by_unit.items():
    print(f"\n{unit}:")
    for i, (crime, prediction) in enumerate(top_crimes, 1):
        print(f"  {i}. {crime}: {prediction:.1f} cases")

# Create pie charts for each unit
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Create subplots for all units
units = list(top_crimes_by_unit.keys())
n_units = len(units)
cols = 3  # Number of columns
rows = (n_units + cols - 1) // cols  # Calculate rows needed

fig = make_subplots(
    rows=rows, cols=cols,
    subplot_titles=[f"{unit} - Top 5 Crimes" for unit in units],
    specs=[[{"type": "pie"}] * cols] * rows
)

# Colors for pie charts
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']

# Add pie charts for each unit
for idx, unit in enumerate(units):
    row = (idx // cols) + 1
    col = (idx % cols) + 1
    
    top_crimes = top_crimes_by_unit[unit]
    crime_names = [crime for crime, _ in top_crimes]
    crime_values = [value for _, value in top_crimes]
    
    fig.add_trace(
        go.Pie(
            labels=crime_names,
            values=crime_values,
            name=unit,
            marker_colors=colors[:len(crime_names)],
            textinfo='label+percent',
            textposition='inside',
            hole=0.3
        ),
        row=row, col=col
    )

# Update layout
fig.update_layout(
    title=f"Top 5 Crime Predictions for {calendar.month_name[next_month]} {next_year} by Police Unit",
    height=300 * rows,
    showlegend=False,
    title_x=0.5
)

# Show the plot
fig.show()

# Create a summary table of top crimes across all units
print(f"\n=== Summary: Most Predicted Crimes for {calendar.month_name[next_month]} {next_year} ===")

# Aggregate predictions across all units for each crime type
crime_totals = {}
for unit, crimes in unit_predictions.items():
    for crime, prediction in crimes.items():
        if crime not in crime_totals:
            crime_totals[crime] = 0
        crime_totals[crime] += prediction

# Get top 10 overall crimes
top_overall_crimes = sorted(crime_totals.items(), key=lambda x: x[1], reverse=True)[:10]

print("\nTop 10 Overall Crime Predictions:")
for i, (crime, total) in enumerate(top_overall_crimes, 1):
    print(f"{i:2d}. {crime}: {total:.1f} cases")

# Create overall pie chart
fig_overall = go.Figure(data=[go.Pie(
    labels=[crime for crime, _ in top_overall_crimes],
    values=[total for _, total in top_overall_crimes],
    marker_colors=colors + ['#DDA0DD', '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E9'],
    textinfo='label+percent',
    textposition='inside',
    hole=0.3
)])

fig_overall.update_layout(
    title=f"Overall Top 10 Crime Predictions for {calendar.month_name[next_month]} {next_year}",
    title_x=0.5
)

fig_overall.show()

"""# **Build Dashboards To Track Crime: Naib & Jobayer**"""

# Save the processed data for future use
Ml_df_time_series_filtered.to_csv('crime_data_processed.csv', index=False)
print("Processed data saved to 'crime_data_processed.csv'")

# Also save the original data for dashboard
df.to_csv('crime_data_original.csv', index=False)
print("Original data saved to 'crime_data_original.csv'")

print("\n" + "="*80)
print("ðŸŽ‰ ALL VISUALIZATIONS AND PREDICTIONS COMPLETED SUCCESSFULLY!")
print("="*80)
print("\nðŸ“Š Next Steps:")
print("1. Run the Streamlit dashboard: streamlit run streamlit_dashboard.py")
print("2. Open your browser to view the interactive dashboard")
print("3. Explore all visualizations and prediction features")
print("="*80)

